# 循环神经网络实现文本情感分类

## 目标

1. 知道LSTM和GRU的使用方法及输入输出的格式
2. 能够应用LSTM和GRU实现文本情感分类



## 1. Pytorch中LSTM和GRU模块使用

### 1.1 LSTM介绍

LSTM和GRU都是由`torch.nn`提供

通过观察文档，可知LSMT的参数，

`torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first,dropout,bidirectional)`

1. `input_size `：输入数据的形状，即embedding_dim
2. `hidden_size`：隐藏层神经元的数量，即每一层有多少个LSTM单元
3. `num_layer` ：即RNN的中LSTM单元的层数
4. `batch_first`：默认值为False，输入的数据需要`[seq_len,batch,feature]`,如果为True，则为`[batch,seq_len,feature]`
5. `dropout`:dropout的比例，默认值为0。dropout是一种训练过程中让部分参数随机失活的一种方式，能够提高训练速度，同时能够解决过拟合的问题。这里是在LSTM的最后一层，对每个输出进行dropout
6. `bidirectional`：是否使用双向LSTM,默认是False

实例化LSTM对象之后,**不仅需要传入数据，还需要前一次的h_0(前一次的隐藏状态)和c_0（前一次memory）**

即：`lstm(input,(h_0,c_0))`

LSTM的默认输出为`output, (h_n, c_n)`

1. `output`：`(seq_len, batch, num_directions * hidden_size)`--->batch_first=False
2. `h_n`:`(num_layers * num_directions, batch, hidden_size)`
3. `c_n`: `(num_layers * num_directions, batch, hidden_size)`

## 1.2 LSTM使用示例

假设数据输入为 input ,形状是`[10,20]`，假设embedding的形状是`[100,30]`

则LSTM使用示例如下：

```python
batch_size =10
seq_len = 20
embedding_dim = 30
word_vocab = 100
hidden_size = 18
num_layer = 2

#准备输入数据
input = torch.randint(low=0,high=100,size=(batch_size,seq_len))
#准备embedding
embedding  = torch.nn.Embedding(word_vocab,embedding_dim)
lstm = torch.nn.LSTM(embedding_dim,hidden_size,num_layer)

#进行mebed操作
embed = embedding(input) #[10,20,30]

#转化数据为batch_first=False
embed = embed.permute(1,0,2) #[20,10,30]

#初始化状态， 如果不初始化，torch默认初始值为全0
h_0 = torch.rand(num_layer,batch_size,hidden_size)
c_0 = torch.rand(num_layer,batch_size,hidden_size)
output,(h_1,c_1) = lstm(embed,(h_0,c_0))
#output [20,10,1*18]
#h_1 [2,10,18]
#c_1 [2,10,18]
```

输出如下

```python
In [122]: output.size()
Out[122]: torch.Size([20, 10, 18])

In [123]: h_1.size()
Out[123]: torch.Size([2, 10, 18])

In [124]: c_1.size()
Out[124]: torch.Size([2, 10, 18])
```

通过前面的学习，我们知道，最后一次的h_1应该和output的最后一个time step的输出是一样的

通过下面的代码，我们来验证一下：

```python
In [179]: a = output[-1,:,:]

In [180]: a.size()
Out[180]: torch.Size([10, 18])

In [183]: b.size()
Out[183]: torch.Size([10, 18])
In [184]: a == b
Out[184]:
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
       dtype=torch.uint8)
```

### 1.3 GRU的使用示例

GRU模块`torch.nn.GRU`，和LSTM的参数相同，含义相同，具体可参考文档

但是输入只剩下`gru(input,h_0)`，输出为`output, h_n`

其形状为：

1. `output`:`(seq_len, batch, num_directions * hidden_size)`
2. `h_n`:`(num_layers * num_directions, batch, hidden_size)`

大家可以使用上述代码，观察GRU的输出形式

### 1.4 双向LSTM

如果需要使用双向LSTM，则在实例化LSTM的过程中，需要把LSTM中的bidriectional设置为True，同时h_0和c_0使用num_layer*2

观察效果，输出为

```python
batch_size =10 #句子的数量
seq_len = 20  #每个句子的长度
embedding_dim = 30  #每个词语使用多长的向量表示
word_vocab = 100  #词典中词语的总数
hidden_size = 18  #隐层中lstm的个数
num_layer = 2  #多少个隐藏层

input = torch.randint(low=0,high=100,size=(batch_size,seq_len))
embedding  = torch.nn.Embedding(word_vocab,embedding_dim)
lstm = torch.nn.LSTM(embedding_dim,hidden_size,num_layer,bidirectional=True)

embed = embedding(input) #[10,20,30]

#转化数据为batch_first=False
embed = embed.permute(1,0,2) #[20,10,30]
h_0 = torch.rand(num_layer*2,batch_size,hidden_size)
c_0 = torch.rand(num_layer*2,batch_size,hidden_size)
output,(h_1,c_1) = lstm(embed,(h_0,c_0))

In [135]: output.size()
Out[135]: torch.Size([20, 10, 36])

In [136]: h_1.size()
Out[136]: torch.Size([4, 10, 18])

In [137]: c_1.size()
Out[137]: torch.Size([4, 10, 18])
```

在单向LSTM中，最后一个time step的输出的前hidden_size个和最后一层隐藏状态h_1的输出相同，那么双向LSTM呢？

双向LSTM中：

**output：按照正反计算的结果顺序在第2个维度进行拼接，正向第一个拼接反向的最后一个输出**

**hidden state:按照得到的结果在第0个维度进行拼接，正向第一个之后接着是反向第一个**

1. 前向的LSTM中，最后一个time step的输出的前hidden_size个和最后一层向前传播h_1的输出相同

   - 示例：

   - ```python
     #-1是前向LSTM的最后一个，前18是前hidden_size个
     In [188]: a = output[-1,:,:18]  #前项LSTM中最后一个time step的output
     
     In [189]: b = h_1[-2,:,:]  #倒数第二个为前向
     
     In [190]: a.size()
     Out[190]: torch.Size([10, 18])
     
     In [191]: b.size()
     Out[191]: torch.Size([10, 18])
     
     In [192]: a == b
     Out[192]:
     tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
            dtype=torch.uint8)
     ```

     

2. 后向LSTM中，最后一个time step的输出的后hidden_size个和最后一层后向传播的h_1的输出相同

   - 示例

   - ```python
     #0 是反向LSTM的最后一个，后18是后hidden_size个
     In [196]: c = output[0,:,18:]  #后向LSTM中的最后一个输出
     
     In [197]: d = h_1[-1,:,:] #后向LSTM中的最后一个隐藏层状态
     
     In [198]: c == d
     Out[198]:
     tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
            dtype=torch.uint8)
     ```



### 1.4 LSTM和GRU的使用注意点

1. 第一次调用之前，需要初始化隐藏状态，如果不初始化，默认创建全为0的隐藏状态
2. 往往会使用LSTM or GRU 的输出的最后一维的结果，来代表LSTM、GRU对文本处理的结果，其形状为`[batch,  num_directions*hidden_size]`。
   1. 并不是所有模型都会使用最后一维的结果
   2. 如果实例化LSTM的过程中，batch_first=False,则`output[-1] or output[-1,:,:]`可以获取最后一维
   3. 如果实例化LSTM的过程中，batch_first=True,则`output[:,-1,:]`可以获取最后一维
3. 如果结果是`(seq_len, batch_size, num_directions * hidden_size)`,需要把它转化为`(batch_size,seq_len, num_directions * hidden_size)`的形状，不能够不是view等变形的方法，需要使用`output.permute(1,0,2)`，即交换0和1轴，实现上述效果
4. 使用双向LSTM的时候，往往会分别使用每个方向最后一次的output，作为当前数据经过双向LSTM的结果
   - 即：`torch.cat([h_1[-2,:,:],h_1[-1,:,:]],dim=-1)`
   - 最后的表示的size是`[batch_size,hidden_size*2]`
5. 上述内容在GRU中同理



## 2. 使用LSTM完成文本情感分类

在前面，我们使用了word embedding去实现了toy级别的文本情感分类，那么现在我们在这个模型中添加上LSTM层，观察分类效果。

为了达到更好的效果，对之前的模型做如下修改

1. MAX_LEN = 200
2.  构建dataset的过程，把数据转化为2分类的问题，pos为1，neg为0，否则25000个样本完成10个类别的划分数据量是不够的
3. 在实例化LSTM的时候，使用dropout=0.5，在model.eval()的过程中，dropout自动会为0

### 2.1 修改模型

```python
class IMDBLstmmodel(nn.Module):
    def __init__(self):
        super(IMDBLstmmodel,self).__init__()
        self.hidden_size = 64
        self.embedding_dim = 200
        self.num_layer = 2
        self.bidriectional = True
        self.bi_num = 2 if self.bidriectional else 1
        self.dropout = 0.5
        #以上部分为超参数，可以自行修改

        self.embedding = nn.Embedding(len(ws),self.embedding_dim,padding_idx=ws.PAD) #[N,300]
        self.lstm = nn.LSTM(self.embedding_dim,self.hidden_size,self.num_layer,bidirectional=True,dropout=self.dropout)
        #使用两个全连接层，中间使用relu激活函数
        self.fc = nn.Linear(self.hidden_size*self.bi_num,20)
        self.fc2 = nn.Linear(20,2)


    def forward(self, x):
        x = self.embedding(x)
        x = x.permute(1,0,2) #进行轴交换
        h_0,c_0 = self.init_hidden_state(x.size(1))
        _,(h_n,c_n) = self.lstm(x,(h_0,c_0))

        #只要最后一个lstm单元处理的结果，这里多去的hidden state
        out = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=-1)
        out = self.fc(out)
        out = F.relu(out)
        out = self.fc2(out)
        return F.log_softmax(out,dim=-1)

    def init_hidden_state(self,batch_size):
        h_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to(device)
        c_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to(device)
        return h_0,c_0
```



### 2.2 完成训练和测试代码

为了提高程序的运行速度，可以考虑把模型放在gup上运行，那么此时需要处理一下几点：

1. `device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`
2. `model.to(device)`
3. 除了上述修改外，涉及计算的所有tensor都需要转化为CUDA的tensor
   1. 初始化的`h_0,c_0`
   2. 训练集和测试集的`input,traget`
4. 在最后可以通过`tensor.cpu()`转化为torch的普通tensor

```python
train_batch_size = 64
test_batch_size = 5000
# imdb_model = IMDBModel(MAX_LEN) #基础model
imdb_model = IMDBLstmmodel().to(device) #在gpu上运行，提高运行速度
# imdb_model.load_state_dict(torch.load("model/mnist_net.pkl"))
optimizer = optim.Adam(imdb_model.parameters())
criterion = nn.CrossEntropyLoss()

def train(epoch):
    mode = True
    imdb_model.train(mode)
    train_dataloader =get_dataloader(mode,train_batch_size)
    for idx,(target,input,input_lenght) in enumerate(train_dataloader):
        target = target.to(device)
        input = input.to(device)
        optimizer.zero_grad()
        output = imdb_model(input)
        loss = F.nll_loss(output,target) #traget需要是[0,9]，不能是[1-10]
        loss.backward()
        optimizer.step()
        if idx %10 == 0:
            pred = torch.max(output, dim=-1, keepdim=False)[-1]
            acc = pred.eq(target.data).cpu().numpy().mean()*100.

            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\t ACC: {:.6f}'.format(epoch, idx * len(input), len(train_dataloader.dataset),
                       100. * idx / len(train_dataloader), loss.item(),acc))

            torch.save(imdb_model.state_dict(), "model/mnist_net.pkl")
            torch.save(optimizer.state_dict(), 'model/mnist_optimizer.pkl')
            
 def test():
    mode = False
    imdb_model.eval()
    test_dataloader = get_dataloader(mode, test_batch_size)
    with torch.no_grad():
        for idx,(target, input, input_lenght) in enumerate(test_dataloader):
            target = target.to(device)
            input = input.to(device)
            output = imdb_model(input)
            test_loss  = F.nll_loss(output, target,reduction="mean")
            pred = torch.max(output,dim=-1,keepdim=False)[-1]
            correct = pred.eq(target.data).sum()
            acc = 100. * pred.eq(target.data).cpu().numpy().mean()
            print('idx: {} Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(idx,test_loss, correct, target.size(0),acc))
            
 if __name__ == "__main__":
    test()
    for i in range(10):
        train(i)
        test()

```

### 2.3 模型训练的最终输出

```
...
Train Epoch: 9 [20480/25000 (82%)]	Loss: 0.017165	 ACC: 100.000000
Train Epoch: 9 [21120/25000 (84%)]	Loss: 0.021572	 ACC: 98.437500
Train Epoch: 9 [21760/25000 (87%)]	Loss: 0.058546	 ACC: 98.437500
Train Epoch: 9 [22400/25000 (90%)]	Loss: 0.045248	 ACC: 98.437500
Train Epoch: 9 [23040/25000 (92%)]	Loss: 0.027622	 ACC: 98.437500
Train Epoch: 9 [23680/25000 (95%)]	Loss: 0.097722	 ACC: 95.312500
Train Epoch: 9 [24320/25000 (97%)]	Loss: 0.026713	 ACC: 98.437500
Train Epoch: 9 [15600/25000 (100%)]	Loss: 0.006082	 ACC: 100.000000
idx: 0 Test set: Avg. loss: 0.8794, Accuracy: 4053/5000 (81.06%)
idx: 1 Test set: Avg. loss: 0.8791, Accuracy: 4018/5000 (80.36%)
idx: 2 Test set: Avg. loss: 0.8250, Accuracy: 4087/5000 (81.74%)
idx: 3 Test set: Avg. loss: 0.8380, Accuracy: 4074/5000 (81.48%)
idx: 4 Test set: Avg. loss: 0.8696, Accuracy: 4027/5000 (80.54%)
```

可以看到模型的测试准确率稳定在81%左右。

大家可以把上述代码改为GRU，或者多层LSTM继续尝试，观察效果